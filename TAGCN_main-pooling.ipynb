{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.modules.module import Module\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import argparse\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.dataloader import _use_shared_memory\n",
    "from torch.nn import Parameter\n",
    "import torchvision.models as models\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import scipy\n",
    "from scipy.sparse import coo_matrix\n",
    "import pdb\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "import os\n",
    "import csv\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.colors as colors\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "from matplotlib.figure import Figure\n",
    "import networkx as nx\n",
    "import sklearn.metrics as metrics\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "import cross_val\n",
    "import encoders\n",
    "import gen.feat as featgen\n",
    "import gen.data as datagen\n",
    "from graph_sampler import GraphSampler\n",
    "import load_data\n",
    "import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Csv_Logger():\n",
    "    def __init__(self, log_path, log_header):\n",
    "        self.log_path = os.path.abspath(log_path)\n",
    "        self.log_header = log_header\n",
    "\n",
    "        self.initialized = False\n",
    "\n",
    "    def _initialize(self):\n",
    "        self.initialized = True\n",
    "        parent_dir = os.path.abspath(os.path.join(self.log_path, '..'))\n",
    "        os.makedirs(parent_dir, exist_ok=True)\n",
    "\n",
    "        with open(self.log_path, \"w\") as f:\n",
    "            writer = csv.DictWriter(f, self.log_header)\n",
    "            writer.writeheader()\n",
    "\n",
    "    def write_row(self, **kwargs):\n",
    "        if not self.initialized:\n",
    "            self._initialize()\n",
    "\n",
    "        with open(self.log_path, \"a\") as f:\n",
    "            writer = csv.DictWriter(f, self.log_header)\n",
    "            writer.writerow(kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    dayX=np.load('../preprocessed_data/daysX.npy').reshape(-1,24,9413).astype(int) #(2738, 9413, 24)\n",
    "    ydaysD=np.load('../preprocessed_data/ydaysD.npy').astype(int) #(2738, 1)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(dayX, ydaysD, test_size=0.2)\n",
    "    y_test=y_test.squeeze()\n",
    "    y_train=y_train.squeeze()\n",
    "    adj_load=scipy.io.loadmat('AdjG.mat')['AdjG']    \n",
    "    adj=adj_load.astype(float)\n",
    "    return adj,x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    # Custom dataset\n",
    "    def __init__(self, features, labels):\n",
    "        labels=labels[:,np.newaxis]\n",
    "        self.features = [torch.from_numpy(x) for x in features]\n",
    "        self.labels = [torch.from_numpy(x) for x in labels]\n",
    "        assert len(self.features) == len(self.labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, item): #for-loop yield \n",
    "        return self.features[item], self.labels[item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adjacency(order_num,adj,device):\n",
    "        #Identity\n",
    "    ret =[]\n",
    "    c=scipy.sparse.identity(9413)\n",
    "    for i in range(order_num+1): #indices, values\n",
    "        temp=coo_matrix(c)\n",
    "        values=temp.data\n",
    "        indices = np.vstack((temp.row, temp.col))\n",
    "        i = torch.LongTensor(indices).to(device)\n",
    "        v = torch.FloatTensor(values).to(device)\n",
    "        adj_v=torch.sparse.FloatTensor(i, v, torch.Size(temp.shape))\n",
    "        adj_v.require_grad=False\n",
    "        adj_v.to(device)\n",
    "        ret.append(adj_v)\n",
    "        c=c*adj\n",
    "    return ret\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TAGCN_layer(Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels,degree,adj_matrices, bias=True,include_identity=False):\n",
    "        super(TAGCN_layer, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.weight = Parameter(torch.FloatTensor(degree+1,in_channels,out_channels))\n",
    "        self.degree=degree\n",
    "        self.bias = Parameter(torch.FloatTensor(out_channels))#parameter and variable\n",
    "        self.reset_parameters_xavier()\n",
    "        self.adj_matrices=adj_matrices\n",
    "        self.include_identity=include_identity\n",
    "\n",
    "        \n",
    "    def reset_parameters_xavier(self):\n",
    "        nn.init.xavier_normal_(self.weight.data,gain=0.02) # Implement Xavier Uniform, 0.02\n",
    "#         nn.init.xavier_uniform_(self.weight.data,gain=0.02) # Implement Xavier Uniform, 0.02\n",
    "\n",
    "        nn.init.constant_(self.bias.data, 0.0)\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # x = inputs # samples by channels by 9413\n",
    "        [number_of_samples,in_channels, number_of_nodes]=inputs.shape\n",
    "        assert (in_channels==self.in_channels)\n",
    "        output=torch.zeros(number_of_nodes,number_of_samples,self.out_channels).to(device)\n",
    "\n",
    "        for out_channel in range(self.out_channels):\n",
    "            for i in range(self.degree):\n",
    "                test=torch.sum(self.weight[i+1,:,out_channel]*inputs.transpose(1,2),dim=2).view(number_of_nodes,-1)\n",
    "                output[:,:,out_channel] += torch.spmm(self.adj_matrices[i+1],test).to(device)\n",
    "\n",
    "            output[:,:,out_channel]=output[:,:,out_channel]+self.bias[out_channel]\n",
    "        return output.view(number_of_samples,self.out_channels,number_of_nodes).squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, optimizer,loader, max_epochs=100,load_path=None):\n",
    "        self.model = model\n",
    "        self.loader = loader\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.predictions = []\n",
    "        self.predictions_test = []\n",
    "        self.generated_logits = []\n",
    "        self.generated = []\n",
    "        self.generated_logits_test = []\n",
    "        self.generated_test = []\n",
    "        self.epochs = 0\n",
    "        self.max_epochs = max_epochs\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.testaccuracies=[]\n",
    "        self.trainaccuracies=[]\n",
    "        self.name='./logs/'+str(time.time())+'.csv'\n",
    "        self.logger = Csv_Logger(self.name, ['episode', 'test_accuracy', 'train_accuracy', 'train_loss'])\n",
    "\n",
    "\n",
    "        if load_path is not None:\n",
    "            self.model.load_state_dict(\n",
    "                torch.load(\n",
    "                    load_path,\n",
    "                    map_location=lambda storage,\n",
    "                    loc: storage))\n",
    "            \n",
    "    def train(self,device):\n",
    "        self.model.train() # set to training mode\n",
    "        epoch_loss = 0\n",
    "        correct = 0\n",
    "        for batch_num, (features,labels) in enumerate(self.loader):\n",
    "            self.optimizer.zero_grad()\n",
    "            bs=features.shape[0]\n",
    "            features=features.to(device).float()\n",
    "            labels=labels.to(device).long().squeeze()\n",
    "            self.model=self.model.to(device)\n",
    "            \n",
    "            out=self.model.forward(features)  \n",
    "            out=out.view(bs,7)\n",
    "            pred = out.data.max(1, keepdim=True)[1]\n",
    "            predicted = pred.eq(labels.data.view_as(pred))\n",
    "            correct += predicted.sum()            \n",
    "            loss = self.criterion(out,labels)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        epoch_loss = epoch_loss / (batch_num + 1)\n",
    "        writer.add_scalar('loss',epoch_loss,self.epochs)\n",
    "\n",
    "        accuracy_train = correct.cpu().numpy() / len(self.loader.dataset)\n",
    "        writer.add_scalar('train accuracy',accuracy_train,self.epochs)\n",
    "\n",
    "        self.epochs += 1\n",
    "        print('[TRAIN]  Epoch [%d/%d]   Loss: %.4f     Accuracy: %.4f'\n",
    "                      % (self.epochs, self.max_epochs, epoch_loss,accuracy_train))\n",
    "\n",
    "        self.train_losses.append(epoch_loss)\n",
    "        self.trainaccuracies.append(accuracy_train)\n",
    "    def save_model(self, path):\n",
    "        torch.save(self.model.state_dict(), path)\n",
    "        \n",
    "    def inference(self,inference_loader,device):\n",
    "        self.model = self.model.eval()\n",
    "        bs=64\n",
    "        y_true=[]\n",
    "        y_pred=[]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            correct=0\n",
    "            for batch_num, (features,labels) in enumerate(inference_loader):\n",
    "#                 self.optimizer.zero_grad()\n",
    "                bs=features.shape[0]\n",
    "                features=features.to(device).float()\n",
    "                labels=labels.to(device).long().squeeze()\n",
    "                self.model=self.model.to(device)\n",
    "                out=self.model.forward(features)  \n",
    "                out=out.view(bs,7)\n",
    "                pred = out.data.max(1, keepdim=True)[1]\n",
    "                predicted = pred.eq(labels.data.view_as(pred))\n",
    "                correct += predicted.sum()\n",
    "                y_true=y_true+labels.cpu().numpy().tolist()\n",
    "                y_pred=y_pred+pred.cpu().numpy().reshape(-1).tolist()\n",
    "            conf_matrix=confusion_matrix(y_true,y_pred)\n",
    "        \n",
    "            accuracy_test = correct.cpu().numpy() / len(inference_loader.dataset)\n",
    "            self.testaccuracies.append(accuracy_test)\n",
    "            print('[Test]  Epoch [%d/%d]   Accuracy: %.4f'\n",
    "                      % (self.epochs, self.max_epochs,accuracy_test))\n",
    "\n",
    "        self.logger.write_row(episode=self.epochs, test_accuracy=accuracy_test,train_accuracy=self.trainaccuracies[-1],train_loss=self.train_losses[-1])\n",
    "\n",
    "#             print('Confusion Matrix: ',conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataset, model, args, name='Validation', max_num_examples=None):\n",
    "    model.eval()\n",
    "\n",
    "    labels = []\n",
    "    preds = []\n",
    "    for batch_idx, data in enumerate(dataset):\n",
    "        adj = Variable(data['adj'].float(), requires_grad=False).cuda()\n",
    "        h0 = Variable(data['feats'].float()).cuda()\n",
    "        labels.append(data['label'].long().numpy())\n",
    "        batch_num_nodes = data['num_nodes'].int().numpy()\n",
    "        assign_input = Variable(data['assign_feats'].float(), requires_grad=False).cuda()\n",
    "\n",
    "        ypred = model(h0, adj, batch_num_nodes, assign_x=assign_input)\n",
    "        _, indices = torch.max(ypred, 1)\n",
    "        preds.append(indices.cpu().data.numpy())\n",
    "\n",
    "        if max_num_examples is not None:\n",
    "            if (batch_idx+1)*args.batch_size > max_num_examples:\n",
    "                break\n",
    "\n",
    "    labels = np.hstack(labels)\n",
    "    preds = np.hstack(preds)\n",
    "    \n",
    "    result = {'prec': metrics.precision_score(labels, preds, average='macro'),\n",
    "              'recall': metrics.recall_score(labels, preds, average='macro'),\n",
    "              'acc': metrics.accuracy_score(labels, preds),\n",
    "              'F1': metrics.f1_score(labels, preds, average=\"micro\")}\n",
    "    print(name, \" accuracy:\", result['acc'])\n",
    "    return result\n",
    "\n",
    "def gen_prefix(args):\n",
    "    if args.bmname is not None:\n",
    "        name = args.bmname\n",
    "    else:\n",
    "        name = args.dataset\n",
    "    name += '_' + args.method\n",
    "    if args.method == 'soft-assign':\n",
    "        name += '_l' + str(args.num_gc_layers) + 'x' + str(args.num_pool)\n",
    "        name += '_ar' + str(int(args.assign_ratio*100))\n",
    "        if args.linkpred:\n",
    "            name += '_lp'\n",
    "    else:\n",
    "        name += '_l' + str(args.num_gc_layers)\n",
    "    name += '_h' + str(args.hidden_dim) + '_o' + str(args.output_dim)\n",
    "    if not args.bias:\n",
    "        name += '_nobias'\n",
    "    if len(args.name_suffix) > 0:\n",
    "        name += '_' + args.name_suffix\n",
    "    return name\n",
    "\n",
    "def gen_train_plt_name(args):\n",
    "    return 'results/' + gen_prefix(args) + '.png'\n",
    "\n",
    "def log_assignment(assign_tensor, writer, epoch, batch_idx):\n",
    "    plt.switch_backend('agg')\n",
    "    fig = plt.figure(figsize=(8,6), dpi=300)\n",
    "\n",
    "    # has to be smaller than args.batch_size\n",
    "    for i in range(len(batch_idx)):\n",
    "        plt.subplot(2, 2, i+1)\n",
    "        plt.imshow(assign_tensor.cpu().data.numpy()[batch_idx[i]], cmap=plt.get_cmap('BuPu'))\n",
    "        cbar = plt.colorbar()\n",
    "        cbar.solids.set_edgecolor(\"face\")\n",
    "    plt.tight_layout()\n",
    "    fig.canvas.draw()\n",
    "\n",
    "    data = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep='')\n",
    "    data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "    writer.add_image('assignment', data, epoch)\n",
    "\n",
    "def log_graph(adj, batch_num_nodes, writer, epoch, batch_idx, assign_tensor=None):\n",
    "    plt.switch_backend('agg')\n",
    "    fig = plt.figure(figsize=(8,6), dpi=300)\n",
    "\n",
    "    for i in range(len(batch_idx)):\n",
    "        ax = plt.subplot(2, 2, i+1)\n",
    "        num_nodes = batch_num_nodes[batch_idx[i]]\n",
    "        adj_matrix = adj[batch_idx[i], :num_nodes, :num_nodes].cpu().data.numpy()\n",
    "        G = nx.from_numpy_matrix(adj_matrix)\n",
    "        nx.draw(G, pos=nx.spring_layout(G), with_labels=True, node_color='#336699',\n",
    "                edge_color='grey', width=0.5, node_size=300,\n",
    "                alpha=0.7)\n",
    "        ax.xaxis.set_visible(False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fig.canvas.draw()\n",
    "\n",
    "    data = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep='')\n",
    "    data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "    writer.add_image('graphs', data, epoch)\n",
    "\n",
    "    # log a label-less version\n",
    "    #fig = plt.figure(figsize=(8,6), dpi=300)\n",
    "    #for i in range(len(batch_idx)):\n",
    "    #    ax = plt.subplot(2, 2, i+1)\n",
    "    #    num_nodes = batch_num_nodes[batch_idx[i]]\n",
    "    #    adj_matrix = adj[batch_idx[i], :num_nodes, :num_nodes].cpu().data.numpy()\n",
    "    #    G = nx.from_numpy_matrix(adj_matrix)\n",
    "    #    nx.draw(G, pos=nx.spring_layout(G), with_labels=False, node_color='#336699',\n",
    "    #            edge_color='grey', width=0.5, node_size=25,\n",
    "    #            alpha=0.8)\n",
    "\n",
    "    #plt.tight_layout()\n",
    "    #fig.canvas.draw()\n",
    "\n",
    "    #data = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep='')\n",
    "    #data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "    #writer.add_image('graphs_no_label', data, epoch)\n",
    "\n",
    "    # colored according to assignment\n",
    "    assignment = assign_tensor.cpu().data.numpy()\n",
    "    fig = plt.figure(figsize=(8,6), dpi=300)\n",
    "\n",
    "    num_clusters = assignment.shape[2]\n",
    "    all_colors = np.array(range(num_clusters))\n",
    "\n",
    "    for i in range(len(batch_idx)):\n",
    "        ax = plt.subplot(2, 2, i+1)\n",
    "        num_nodes = batch_num_nodes[batch_idx[i]]\n",
    "        adj_matrix = adj[batch_idx[i], :num_nodes, :num_nodes].cpu().data.numpy()\n",
    "\n",
    "        label = np.argmax(assignment[batch_idx[i]], axis=1).astype(int)\n",
    "        label = label[: batch_num_nodes[batch_idx[i]]]\n",
    "        node_colors = all_colors[label]\n",
    "\n",
    "        G = nx.from_numpy_matrix(adj_matrix)\n",
    "        nx.draw(G, pos=nx.spring_layout(G), with_labels=False, node_color=node_colors,\n",
    "                edge_color='grey', width=0.4, node_size=50, cmap=plt.get_cmap('Set1'),\n",
    "                vmin=0, vmax=num_clusters-1,\n",
    "                alpha=0.8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fig.canvas.draw()\n",
    "\n",
    "    data = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep='')\n",
    "    data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "    writer.add_image('graphs_colored', data, epoch)\n",
    "\n",
    "\n",
    "def train(dataset, model, args, same_feat=True, val_dataset=None, test_dataset=None, writer=None,\n",
    "        mask_nodes = True):\n",
    "    writer_batch_idx = [0, 3, 6, 9]\n",
    "    \n",
    "    optimizer = torch.optim.Adam(filter(lambda p : p.requires_grad, model.parameters()), lr=0.001)\n",
    "    iter = 0\n",
    "    best_val_result = {\n",
    "            'epoch': 0,\n",
    "            'loss': 0,\n",
    "            'acc': 0}\n",
    "    test_result = {\n",
    "            'epoch': 0,\n",
    "            'loss': 0,\n",
    "            'acc': 0}\n",
    "    train_accs = []\n",
    "    train_epochs = []\n",
    "    best_val_accs = []\n",
    "    best_val_epochs = []\n",
    "    test_accs = []\n",
    "    test_epochs = []\n",
    "    val_accs = []\n",
    "    for epoch in range(args.num_epochs):\n",
    "        begin_time = time.time()\n",
    "        avg_loss = 0.0\n",
    "        model.train()\n",
    "        print('Epoch: ', epoch)\n",
    "        for batch_idx, data in enumerate(dataset):\n",
    "            model.zero_grad()\n",
    "            adj = Variable(data['adj'].float(), requires_grad=False).cuda()\n",
    "            h0 = Variable(data['feats'].float(), requires_grad=False).cuda()\n",
    "            label = Variable(data['label'].long()).cuda()\n",
    "            batch_num_nodes = data['num_nodes'].int().numpy() if mask_nodes else None\n",
    "            assign_input = Variable(data['assign_feats'].float(), requires_grad=False).cuda()\n",
    "\n",
    "            ypred = model(h0, adj, batch_num_nodes, assign_x=assign_input)\n",
    "            if not args.method == 'soft-assign' or not args.linkpred:\n",
    "                loss = model.loss(ypred, label)\n",
    "            else:\n",
    "                loss = model.loss(ypred, label, adj, batch_num_nodes)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
    "            optimizer.step()\n",
    "            iter += 1\n",
    "            avg_loss += loss\n",
    "            #if iter % 20 == 0:\n",
    "            #    print('Iter: ', iter, ', loss: ', loss.data[0])\n",
    "\n",
    "            # log once per XX epochs\n",
    "            if epoch % 10 == 0 and batch_idx == len(dataset) // 2 and args.method == 'soft-assign' and writer is not None:\n",
    "                log_assignment(model.assign_tensor, writer, epoch, writer_batch_idx)\n",
    "                log_graph(adj, batch_num_nodes, writer, epoch, writer_batch_idx, model.assign_tensor)\n",
    "        avg_loss /= batch_idx + 1\n",
    "        elapsed = time.time() - begin_time\n",
    "        if writer is not None:\n",
    "            writer.add_scalar('loss/avg_loss', avg_loss, epoch)\n",
    "            if args.linkpred:\n",
    "                writer.add_scalar('loss/linkpred_loss', model.link_loss, epoch)\n",
    "        print('Avg loss: ', avg_loss, '; epoch time: ', elapsed)\n",
    "        result = evaluate(dataset, model, args, name='Train', max_num_examples=100)\n",
    "        train_accs.append(result['acc'])\n",
    "        train_epochs.append(epoch)\n",
    "        if val_dataset is not None:\n",
    "            val_result = evaluate(val_dataset, model, args, name='Validation')\n",
    "            val_accs.append(val_result['acc'])\n",
    "        if val_result['acc'] > best_val_result['acc'] - 1e-7:\n",
    "            best_val_result['acc'] = val_result['acc']\n",
    "            best_val_result['epoch'] = epoch\n",
    "            best_val_result['loss'] = avg_loss\n",
    "        if test_dataset is not None:\n",
    "            test_result = evaluate(test_dataset, model, args, name='Test')\n",
    "            test_result['epoch'] = epoch\n",
    "        if writer is not None:\n",
    "            writer.add_scalar('acc/train_acc', result['acc'], epoch)\n",
    "            writer.add_scalar('acc/val_acc', val_result['acc'], epoch)\n",
    "            writer.add_scalar('loss/best_val_loss', best_val_result['loss'], epoch)\n",
    "            if test_dataset is not None:\n",
    "                writer.add_scalar('acc/test_acc', test_result['acc'], epoch)\n",
    "\n",
    "        print('Best val result: ', best_val_result)\n",
    "        best_val_epochs.append(best_val_result['epoch'])\n",
    "        best_val_accs.append(best_val_result['acc'])\n",
    "        if test_dataset is not None:\n",
    "            print('Test result: ', test_result)\n",
    "            test_epochs.append(test_result['epoch'])\n",
    "            test_accs.append(test_result['acc'])\n",
    "\n",
    "    matplotlib.style.use('seaborn')\n",
    "    plt.switch_backend('agg')\n",
    "    plt.figure()\n",
    "    plt.plot(train_epochs, util.exp_moving_avg(train_accs, 0.85), '-', lw=1)\n",
    "    if test_dataset is not None:\n",
    "        plt.plot(best_val_epochs, best_val_accs, 'bo', test_epochs, test_accs, 'go')\n",
    "        plt.legend(['train', 'val', 'test'])\n",
    "    else:\n",
    "        plt.plot(best_val_epochs, best_val_accs, 'bo')\n",
    "        plt.legend(['train', 'val'])\n",
    "    plt.savefig(gen_train_plt_name(args), dpi=600)\n",
    "    plt.close()\n",
    "    matplotlib.style.use('default')\n",
    "\n",
    "    return model, val_accs\n",
    "\n",
    "def prepare_data(graphs, args, test_graphs=None, max_nodes=0):\n",
    "\n",
    "    random.shuffle(graphs)\n",
    "    if test_graphs is None:\n",
    "        train_idx = int(len(graphs) * args.train_ratio)\n",
    "        test_idx = int(len(graphs) * (1-args.test_ratio))\n",
    "        train_graphs = graphs[:train_idx]\n",
    "        val_graphs = graphs[train_idx: test_idx]\n",
    "        test_graphs = graphs[test_idx:]\n",
    "    else:\n",
    "        train_idx = int(len(graphs) * args.train_ratio)\n",
    "        train_graphs = graphs[:train_idx]\n",
    "        val_graphs = graph[train_idx:]\n",
    "    print('Num training graphs: ', len(train_graphs), \n",
    "          '; Num validation graphs: ', len(val_graphs),\n",
    "          '; Num testing graphs: ', len(test_graphs))\n",
    "\n",
    "    print('Number of graphs: ', len(graphs))\n",
    "    print('Number of edges: ', sum([G.number_of_edges() for G in graphs]))\n",
    "    print('Max, avg, std of graph size: ', \n",
    "            max([G.number_of_nodes() for G in graphs]), ', '\n",
    "            \"{0:.2f}\".format(np.mean([G.number_of_nodes() for G in graphs])), ', '\n",
    "            \"{0:.2f}\".format(np.std([G.number_of_nodes() for G in graphs])))\n",
    "\n",
    "    # minibatch\n",
    "    dataset_sampler = GraphSampler(train_graphs, normalize=False, max_num_nodes=max_nodes,\n",
    "            features=args.feature_type)\n",
    "    train_dataset_loader = torch.utils.data.DataLoader(\n",
    "            dataset_sampler, \n",
    "            batch_size=args.batch_size, \n",
    "            shuffle=True,\n",
    "            num_workers=args.num_workers)\n",
    "\n",
    "    dataset_sampler = GraphSampler(val_graphs, normalize=False, max_num_nodes=max_nodes,\n",
    "            features=args.feature_type)\n",
    "    val_dataset_loader = torch.utils.data.DataLoader(\n",
    "            dataset_sampler, \n",
    "            batch_size=args.batch_size, \n",
    "            shuffle=False,\n",
    "            num_workers=args.num_workers)\n",
    "\n",
    "    dataset_sampler = GraphSampler(test_graphs, normalize=False, max_num_nodes=max_nodes,\n",
    "            features=args.feature_type)\n",
    "    test_dataset_loader = torch.utils.data.DataLoader(\n",
    "            dataset_sampler, \n",
    "            batch_size=args.batch_size, \n",
    "            shuffle=False,\n",
    "            num_workers=args.num_workers)\n",
    "\n",
    "    return train_dataset_loader, val_dataset_loader, test_dataset_loader, \\\n",
    "            dataset_sampler.max_num_nodes, dataset_sampler.feat_dim, dataset_sampler.assign_feat_dim\n",
    "\n",
    "def syn_community1v2(args, writer=None, export_graphs=False):\n",
    "\n",
    "    # data\n",
    "    graphs1 = datagen.gen_ba(range(40, 60), range(4, 5), 500, \n",
    "            featgen.ConstFeatureGen(np.ones(args.input_dim, dtype=float)))\n",
    "    for G in graphs1:\n",
    "        G.graph['label'] = 0\n",
    "    if export_graphs:\n",
    "        util.draw_graph_list(graphs1[:16], 4, 4, 'figs/ba')\n",
    "\n",
    "    graphs2 = datagen.gen_2community_ba(range(20, 30), range(4, 5), 500, 0.3, \n",
    "            [featgen.ConstFeatureGen(np.ones(args.input_dim, dtype=float))])\n",
    "    for G in graphs2:\n",
    "        G.graph['label'] = 1\n",
    "    if export_graphs:\n",
    "        util.draw_graph_list(graphs2[:16], 4, 4, 'figs/ba2')\n",
    "\n",
    "    graphs = graphs1 + graphs2\n",
    "    \n",
    "    train_dataset, val_dataset, test_dataset, max_num_nodes, input_dim, assign_input_dim = prepare_data(graphs, args)\n",
    "    if args.method == 'soft-assign':\n",
    "        print('Method: soft-assign')\n",
    "        model = encoders.SoftPoolingGcnEncoder(\n",
    "                max_num_nodes, \n",
    "                input_dim, args.hidden_dim, args.output_dim, args.num_classes, args.num_gc_layers,\n",
    "                args.hidden_dim, assign_ratio=args.assign_ratio, num_pooling=args.num_pool,\n",
    "                bn=args.bn, linkpred=args.linkpred, assign_input_dim=assign_input_dim).cuda()\n",
    "    elif args.method == 'base-set2set':\n",
    "        print('Method: base-set2set')\n",
    "        model = encoders.GcnSet2SetEncoder(input_dim, args.hidden_dim, args.output_dim, 2,\n",
    "                args.num_gc_layers, bn=args.bn).cuda()\n",
    "    else:\n",
    "        print('Method: base')\n",
    "        model = encoders.GcnEncoderGraph(input_dim, args.hidden_dim, args.output_dim, 2,\n",
    "                args.num_gc_layers, bn=args.bn).cuda()\n",
    "\n",
    "    train(train_dataset, model, args, val_dataset=val_dataset, test_dataset=test_dataset,\n",
    "            writer=writer)\n",
    "\n",
    "def syn_community2hier(args, writer=None):\n",
    "\n",
    "    # data\n",
    "    feat_gen = [featgen.ConstFeatureGen(np.ones(args.input_dim, dtype=float))]\n",
    "    graphs1 = datagen.gen_2hier(1000, [2,4], 10, range(4,5), 0.1, 0.03, feat_gen)\n",
    "    graphs2 = datagen.gen_2hier(1000, [3,3], 10, range(4,5), 0.1, 0.03, feat_gen)\n",
    "    graphs3 = datagen.gen_2community_ba(range(28, 33), range(4,7), 1000, 0.25, feat_gen)\n",
    "\n",
    "    for G in graphs1:\n",
    "        G.graph['label'] = 0\n",
    "    for G in graphs2:\n",
    "        G.graph['label'] = 1\n",
    "    for G in graphs3:\n",
    "        G.graph['label'] = 2\n",
    "\n",
    "    graphs = graphs1 + graphs2 + graphs3\n",
    "\n",
    "    train_dataset, val_dataset, test_dataset, max_num_nodes, input_dim, assign_input_dim = prepare_data(graphs, args)\n",
    "\n",
    "    if args.method == 'soft-assign':\n",
    "        print('Method: soft-assign')\n",
    "        model = encoders.SoftPoolingGcnEncoder(\n",
    "                max_num_nodes, \n",
    "                input_dim, args.hidden_dim, args.output_dim, args.num_classes, args.num_gc_layers,\n",
    "                args.hidden_dim, assign_ratio=args.assign_ratio, num_pooling=args.num_pool,\n",
    "                bn=args.bn, linkpred=args.linkpred, args=args, assign_input_dim=assign_input_dim).cuda()\n",
    "    elif args.method == 'base-set2set':\n",
    "        print('Method: base-set2set')\n",
    "        model = encoders.GcnSet2SetEncoder(input_dim, args.hidden_dim, args.output_dim, 2,\n",
    "                args.num_gc_layers, bn=args.bn, args=args, assign_input_dim=assign_input_dim).cuda()\n",
    "    else:\n",
    "        print('Method: base')\n",
    "        model = encoders.GcnEncoderGraph(input_dim, args.hidden_dim, args.output_dim, 2,\n",
    "                args.num_gc_layers, bn=args.bn, args=args).cuda()\n",
    "    train(train_dataset, model, args, val_dataset=val_dataset, test_dataset=test_dataset,\n",
    "            writer=writer)\n",
    "\n",
    "\n",
    "def pkl_task(args, feat=None):\n",
    "    with open(os.path.join(args.datadir, args.pkl_fname), 'rb') as pkl_file:\n",
    "        data = pickle.load(pkl_file)\n",
    "    graphs = data[0]\n",
    "    labels = data[1]\n",
    "    test_graphs = data[2]\n",
    "    test_labels = data[3]\n",
    "\n",
    "    for i in range(len(graphs)):\n",
    "        graphs[i].graph['label'] = labels[i]\n",
    "    for i in range(len(test_graphs)):\n",
    "        test_graphs[i].graph['label'] = test_labels[i]\n",
    "\n",
    "    if feat is None:\n",
    "        featgen_const = featgen.ConstFeatureGen(np.ones(args.input_dim, dtype=float))\n",
    "        for G in graphs:\n",
    "            featgen_const.gen_node_features(G)\n",
    "        for G in test_graphs:\n",
    "            featgen_const.gen_node_features(G)\n",
    "\n",
    "    train_dataset, test_dataset, max_num_nodes = prepare_data(graphs, args, test_graphs=test_graphs)\n",
    "    model = encoders.GcnEncoderGraph(\n",
    "            args.input_dim, args.hidden_dim, args.output_dim, args.num_classes, \n",
    "            args.num_gc_layers, bn=args.bn).cuda()\n",
    "    train(train_dataset, model, args, test_dataset=test_dataset)\n",
    "    evaluate(test_dataset, model, args, 'Validation')\n",
    "\n",
    "def benchmark_task(args, writer=None, feat='node-label'):\n",
    "    graphs = load_data.read_graphfile(args.datadir, args.bmname, max_nodes=args.max_nodes)\n",
    "    \n",
    "    if feat == 'node-feat' and 'feat_dim' in graphs[0].graph:\n",
    "        print('Using node features')\n",
    "        input_dim = graphs[0].graph['feat_dim']\n",
    "    elif feat == 'node-label' and 'label' in graphs[0].node[0]:\n",
    "        print('Using node labels')\n",
    "        for G in graphs:\n",
    "            for u in G.nodes():\n",
    "                G.node[u]['feat'] = np.array(G.node[u]['label'])\n",
    "    else:\n",
    "        print('Using constant labels')\n",
    "        featgen_const = featgen.ConstFeatureGen(np.ones(args.input_dim, dtype=float))\n",
    "        for G in graphs:\n",
    "            featgen_const.gen_node_features(G)\n",
    "\n",
    "    train_dataset, val_dataset, test_dataset, max_num_nodes, input_dim, assign_input_dim = \\\n",
    "            prepare_data(graphs, args, max_nodes=args.max_nodes)\n",
    "    if args.method == 'soft-assign':\n",
    "        print('Method: soft-assign')\n",
    "        model = encoders.SoftPoolingGcnEncoder(\n",
    "                max_num_nodes, \n",
    "                input_dim, args.hidden_dim, args.output_dim, args.num_classes, args.num_gc_layers,\n",
    "                args.hidden_dim, assign_ratio=args.assign_ratio, num_pooling=args.num_pool,\n",
    "                bn=args.bn, dropout=args.dropout, linkpred=args.linkpred, args=args,\n",
    "                assign_input_dim=assign_input_dim).cuda()\n",
    "    elif args.method == 'base-set2set':\n",
    "        print('Method: base-set2set')\n",
    "        model = encoders.GcnSet2SetEncoder(\n",
    "                input_dim, args.hidden_dim, args.output_dim, args.num_classes,\n",
    "                args.num_gc_layers, bn=args.bn, dropout=args.dropout, args=args).cuda()\n",
    "    else:\n",
    "        print('Method: base')\n",
    "        model = encoders.GcnEncoderGraph(\n",
    "                input_dim, args.hidden_dim, args.output_dim, args.num_classes, \n",
    "                args.num_gc_layers, bn=args.bn, dropout=args.dropout, args=args).cuda()\n",
    "\n",
    "    train(train_dataset, model, args, val_dataset=val_dataset, test_dataset=test_dataset,\n",
    "            writer=writer)\n",
    "    evaluate(test_dataset, model, args, 'Validation')\n",
    "\n",
    "\n",
    "def benchmark_task_val(args, writer=None, feat='node-label'):\n",
    "    all_vals = []\n",
    "    graphs = load_data.read_graphfile(args.datadir, args.bmname, max_nodes=args.max_nodes)\n",
    "    \n",
    "    if feat == 'node-feat' and 'feat_dim' in graphs[0].graph:\n",
    "        print('Using node features')\n",
    "        input_dim = graphs[0].graph['feat_dim']\n",
    "    elif feat == 'node-label' and 'label' in graphs[0].node[0]:\n",
    "        print('Using node labels')\n",
    "        for G in graphs:\n",
    "            for u in G.nodes():\n",
    "                G.node[u]['feat'] = np.array(G.node[u]['label'])\n",
    "    else:\n",
    "        print('Using constant labels')\n",
    "        featgen_const = featgen.ConstFeatureGen(np.ones(args.input_dim, dtype=float))\n",
    "        for G in graphs:\n",
    "            featgen_const.gen_node_features(G)\n",
    "\n",
    "    for i in range(10):\n",
    "        train_dataset, val_dataset, max_num_nodes, input_dim, assign_input_dim = \\\n",
    "                cross_val.prepare_val_data(graphs, args, i, max_nodes=args.max_nodes)\n",
    "        if args.method == 'soft-assign':\n",
    "            print('Method: soft-assign')\n",
    "            model = encoders.SoftPoolingGcnEncoder(\n",
    "                    max_num_nodes, \n",
    "                    input_dim, args.hidden_dim, args.output_dim, args.num_classes, args.num_gc_layers,\n",
    "                    args.hidden_dim, assign_ratio=args.assign_ratio, num_pooling=args.num_pool,\n",
    "                    bn=args.bn, dropout=args.dropout, linkpred=args.linkpred, args=args,\n",
    "                    assign_input_dim=assign_input_dim).cuda()\n",
    "        elif args.method == 'base-set2set':\n",
    "            print('Method: base-set2set')\n",
    "            model = encoders.GcnSet2SetEncoder(\n",
    "                    input_dim, args.hidden_dim, args.output_dim, args.num_classes,\n",
    "                    args.num_gc_layers, bn=args.bn, dropout=args.dropout, args=args).cuda()\n",
    "        else:\n",
    "            print('Method: base')\n",
    "            model = encoders.GcnEncoderGraph(\n",
    "                    input_dim, args.hidden_dim, args.output_dim, args.num_classes, \n",
    "                    args.num_gc_layers, bn=args.bn, dropout=args.dropout, args=args).cuda()\n",
    "\n",
    "        _, val_accs = train(train_dataset, model, args, val_dataset=val_dataset, test_dataset=None,\n",
    "            writer=writer)\n",
    "        all_vals.append(np.array(val_accs))\n",
    "    all_vals = np.vstack(all_vals)\n",
    "    all_vals = np.mean(all_vals, axis=0)\n",
    "    print(all_vals)\n",
    "    print(np.max(all_vals))\n",
    "    print(np.argmax(all_vals))\n",
    "    \n",
    "    \n",
    "def arg_parse():\n",
    "    parser = argparse.ArgumentParser(description='GraphPool arguments.')\n",
    "    io_parser = parser.add_mutually_exclusive_group(required=False)\n",
    "    io_parser.add_argument('--dataset', dest='dataset', \n",
    "            help='Input dataset.')\n",
    "    benchmark_parser = io_parser.add_argument_group()\n",
    "    benchmark_parser.add_argument('--bmname', dest='bmname',\n",
    "            help='Name of the benchmark dataset')\n",
    "    io_parser.add_argument('--pkl', dest='pkl_fname',\n",
    "            help='Name of the pkl data file')\n",
    "\n",
    "    softpool_parser = parser.add_argument_group()\n",
    "    softpool_parser.add_argument('--assign-ratio', dest='assign_ratio', type=float,\n",
    "            help='ratio of number of nodes in consecutive layers')\n",
    "    softpool_parser.add_argument('--num-pool', dest='num_pool', type=int,\n",
    "            help='number of pooling layers')\n",
    "    parser.add_argument('--linkpred', dest='linkpred', action='store_const',\n",
    "            const=True, default=False,\n",
    "            help='Whether link prediction side objective is used')\n",
    "\n",
    "\n",
    "    parser.add_argument('--datadir', dest='datadir',\n",
    "            help='Directory where benchmark is located')\n",
    "    parser.add_argument('--logdir', dest='logdir',\n",
    "            help='Tensorboard log directory')\n",
    "    parser.add_argument('--cuda', dest='cuda',\n",
    "            help='CUDA.')\n",
    "    parser.add_argument('--max-nodes', dest='max_nodes', type=int,\n",
    "            help='Maximum number of nodes (ignore graghs with nodes exceeding the number.')\n",
    "    parser.add_argument('--lr', dest='lr', type=float,\n",
    "            help='Learning rate.')\n",
    "    parser.add_argument('--clip', dest='clip', type=float,\n",
    "            help='Gradient clipping.')\n",
    "    parser.add_argument('--batch-size', dest='batch_size', type=int,\n",
    "            help='Batch size.')\n",
    "    parser.add_argument('--epochs', dest='num_epochs', type=int,\n",
    "            help='Number of epochs to train.')\n",
    "    parser.add_argument('--train-ratio', dest='train_ratio', type=float,\n",
    "            help='Ratio of number of graphs training set to all graphs.')\n",
    "    parser.add_argument('--num_workers', dest='num_workers', type=int,\n",
    "            help='Number of workers to load data.')\n",
    "    parser.add_argument('--feature', dest='feature_type',\n",
    "            help='Feature used for encoder. Can be: id, deg')\n",
    "    parser.add_argument('--input-dim', dest='input_dim', type=int,\n",
    "            help='Input feature dimension')\n",
    "    parser.add_argument('--hidden-dim', dest='hidden_dim', type=int,\n",
    "            help='Hidden dimension')\n",
    "    parser.add_argument('--output-dim', dest='output_dim', type=int,\n",
    "            help='Output dimension')\n",
    "    parser.add_argument('--num-classes', dest='num_classes', type=int,\n",
    "            help='Number of label classes')\n",
    "    parser.add_argument('--num-gc-layers', dest='num_gc_layers', type=int,\n",
    "            help='Number of graph convolution layers before each pooling')\n",
    "    parser.add_argument('--nobn', dest='bn', action='store_const',\n",
    "            const=False, default=True,\n",
    "            help='Whether batch normalization is used')\n",
    "    parser.add_argument('--dropout', dest='dropout', type=float,\n",
    "            help='Dropout rate.')\n",
    "    parser.add_argument('--nobias', dest='bias', action='store_const',\n",
    "            const=False, default=True,\n",
    "            help='Whether to add bias. Default to True.')\n",
    "\n",
    "    parser.add_argument('--method', dest='method',\n",
    "            help='Method. Possible values: base, base-set2set, soft-assign')\n",
    "    parser.add_argument('--name-suffix', dest='name_suffix',\n",
    "            help='suffix added to the output filename')\n",
    "\n",
    "    parser.set_defaults(datadir='data',\n",
    "                        logdir='log',\n",
    "                        dataset='syn1v2',\n",
    "                        max_nodes=1000,\n",
    "                        cuda='1',\n",
    "                        feature_type='default',\n",
    "                        lr=0.001,\n",
    "                        clip=2.0,\n",
    "                        batch_size=20,\n",
    "                        num_epochs=1000,\n",
    "                        train_ratio=0.8,\n",
    "                        test_ratio=0.1,\n",
    "                        num_workers=1,\n",
    "                        input_dim=10,\n",
    "                        hidden_dim=20,\n",
    "                        output_dim=20,\n",
    "                        num_classes=2,\n",
    "                        num_gc_layers=3,\n",
    "                        dropout=0.0,\n",
    "                        method='base',\n",
    "                        name_suffix='',\n",
    "                        assign_ratio=0.1,\n",
    "                        num_pool=1\n",
    "                       )\n",
    "    return parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove existing log dir:  log/syn1v2_base_l3_h20_o20_nobias\n",
      "CUDA 1\n",
      "Num training graphs:  800 ; Num validation graphs:  100 ; Num testing graphs:  100\n",
      "Number of graphs:  1000\n",
      "Number of edges:  176060\n",
      "Max, avg, std of graph size:  59 , 49.10 , 4.90\n",
      "Method: base\n",
      "Epoch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mark/Ph.D. Research/Projects/GCN/gcn_env/lib/python3.6/site-packages/ipykernel_launcher.py:181: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss:  tensor(0.6946, device='cuda:0', grad_fn=<DivBackward0>) ; epoch time:  0.7390921115875244\n",
      "Train  accuracy: 0.49166666666666664\n",
      "Validation  accuracy: 0.47\n",
      "Test  accuracy: 0.42\n",
      "Best val result:  {'epoch': 0, 'loss': tensor(0.6946, device='cuda:0', grad_fn=<DivBackward0>), 'acc': 0.47}\n",
      "Test result:  {'prec': 0.21, 'recall': 0.5, 'acc': 0.42, 'F1': 0.41999999999999993, 'epoch': 0}\n",
      "Epoch:  1\n"
     ]
    }
   ],
   "source": [
    "# export scalar data to JSON for external processing\n",
    "class prog_args_init:\n",
    "    def __init__(self):\n",
    "        self.datadir='data'\n",
    "        self.logdir='log'\n",
    "        self.dataset='syn1v2'\n",
    "        self.max_nodes=1000\n",
    "        self.cuda='1'\n",
    "        self.feature_type='default'\n",
    "        self.lr=0.001\n",
    "        self.clip=2.0\n",
    "        self.batch_size=20\n",
    "        self.num_epochs=1000\n",
    "        self.train_ratio=0.8\n",
    "        self.test_ratio=0.1\n",
    "        self.num_workers=0\n",
    "        self.input_dim=10\n",
    "        self.hidden_dim=20\n",
    "        self.output_dim=20\n",
    "        self.num_classes=2\n",
    "        self.num_gc_layers=3\n",
    "        self.dropout=0.0\n",
    "        self.method='base'\n",
    "        self.name_suffix=''\n",
    "        self.assign_ratio=0.1\n",
    "        self.num_pool=1\n",
    "        self.bmname=None\n",
    "        self.bias=None\n",
    "        self.pkl_fname=None\n",
    "        self.bn=None\n",
    "        self.linkpred=None\n",
    "prog_args=prog_args_init()     \n",
    "path = os.path.join(prog_args.logdir, gen_prefix(prog_args))\n",
    "if os.path.isdir(path):\n",
    "    print('Remove existing log dir: ', path)\n",
    "    shutil.rmtree(path)\n",
    "writer = SummaryWriter(path)\n",
    "#writer = None\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = prog_args.cuda\n",
    "print('CUDA', prog_args.cuda)\n",
    "\n",
    "if prog_args.bmname is not None:\n",
    "    benchmark_task_val(prog_args, writer=writer)\n",
    "elif prog_args.pkl_fname is not None:\n",
    "    pkl_task(prog_args)\n",
    "elif prog_args.dataset is not None:\n",
    "    if prog_args.dataset == 'syn1v2':\n",
    "        syn_community1v2(prog_args, writer=writer)\n",
    "    if prog_args.dataset == 'syn2hier':\n",
    "        syn_community2hier(prog_args, writer=writer)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adj,x_train, x_test, y_train, y_test=load_data()\n",
    "# number_of_nodes=9413\n",
    "# number_of_classes=7\n",
    "# writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #hyperparameters\n",
    "# EPOCHS=400\n",
    "# lr=1e-3\n",
    "# # wd=1.2e-6\n",
    "# # wd=1e-6\n",
    "# # batch_size=2048\n",
    "# batch_size=256\n",
    "# include_identity=False \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# train_dataset=Dataset(x_train,y_train)\n",
    "# train_loader = DataLoader(train_dataset,shuffle=True, batch_size=batch_size)\n",
    "# test_dataset=Dataset(x_test,y_test)\n",
    "# test_loader = DataLoader(test_dataset,shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degree:  3\n",
      "Number of parameters:  65995\n",
      "[TRAIN]  Epoch [1/400]   Loss: 2.6816     Accuracy: 0.1479\n",
      "[Test]  Epoch [1/400]   Accuracy: 0.2336\n",
      "8.668795108795166\n",
      "[TRAIN]  Epoch [2/400]   Loss: 2.2607     Accuracy: 0.2489\n",
      "[Test]  Epoch [2/400]   Accuracy: 0.2920\n",
      "7.962458848953247\n",
      "[TRAIN]  Epoch [3/400]   Loss: 1.9454     Accuracy: 0.3425\n",
      "[Test]  Epoch [3/400]   Accuracy: 0.3120\n",
      "7.862977981567383\n",
      "[TRAIN]  Epoch [4/400]   Loss: 1.5665     Accuracy: 0.4251\n",
      "[Test]  Epoch [4/400]   Accuracy: 0.4051\n",
      "7.885333299636841\n",
      "[TRAIN]  Epoch [5/400]   Loss: 1.3249     Accuracy: 0.4699\n",
      "[Test]  Epoch [5/400]   Accuracy: 0.5128\n",
      "7.427367925643921\n",
      "[TRAIN]  Epoch [6/400]   Loss: 1.2250     Accuracy: 0.5187\n",
      "[Test]  Epoch [6/400]   Accuracy: 0.5894\n",
      "5.840807676315308\n",
      "[TRAIN]  Epoch [7/400]   Loss: 1.1303     Accuracy: 0.5758\n",
      "[Test]  Epoch [7/400]   Accuracy: 0.6168\n",
      "5.839746713638306\n",
      "[TRAIN]  Epoch [8/400]   Loss: 1.0714     Accuracy: 0.6073\n",
      "[Test]  Epoch [8/400]   Accuracy: 0.5949\n",
      "5.816826105117798\n",
      "[TRAIN]  Epoch [9/400]   Loss: 0.9759     Accuracy: 0.6306\n",
      "[Test]  Epoch [9/400]   Accuracy: 0.6350\n",
      "5.820000648498535\n"
     ]
    }
   ],
   "source": [
    "# epoch=0\n",
    "# for degree in [3]: #,3,4 #1,2,3,4\n",
    "# #     for dropout_rate in [0.2,0.5,0.8]:\n",
    "#     print('Degree: ', degree)\n",
    "#     adj_matrices=get_adjacency(degree,adj,device)\n",
    "#     model=nn.Sequential(TAGCN_layer(24,1,degree,adj_matrices,include_identity),nn.ELU(),nn.Linear(number_of_nodes,number_of_classes)) \n",
    "# #     model=nn.Sequential(TAGCN_layer(24,1,degree,adj_matrices,include_identity),nn.ReLU(),nn.BatchNorm1d(number_of_nodes), nn.Linear(number_of_nodes,number_of_classes)) \n",
    "# #     model=nn.Sequential(TAGCN_layer(24,1,degree,adj_matrices,include_identity),nn.ReLU(),nn.Linear(number_of_nodes,number_of_classes)) \n",
    "\n",
    "    \n",
    "# #     model=nn.Sequential(TAGCN_layer(24,24,degree,adj_matrices,include_identity),nn.ELU(),TAGCN_layer(24,8,degree,adj_matrices,include_identity),nn.ELU(),TAGCN_layer(8,1,degree,adj_matrices,include_identity),nn.ELU(),nn.Linear(number_of_nodes,number_of_classes)) \n",
    "# #     model=nn.Sequential(TAGCN_layer(24,1,degree,adj_matrices,include_identity),nn.ELU(),nn.Linear(number_of_nodes,number_of_classes)) \n",
    "#     print('Number of parameters: ', count_parameters(model))\n",
    "#     AdamOptimizer = torch.optim.Adam(model.parameters())\n",
    "#     trainer=Trainer(model,AdamOptimizer,train_loader, max_epochs=EPOCHS)\n",
    "#     epoch=0\n",
    "#     while epoch<EPOCHS:\n",
    "#         ts=time.time()\n",
    "#         epoch+=1\n",
    "#         trainer.train(device)\n",
    "#         trainer.inference(test_loader,device)\n",
    "#         te=time.time()\n",
    "#         print(te-ts)\n",
    "\n",
    "# #     del adj_matrices\n",
    "\n",
    "# #     print(np.mean(trainer.testaccuracies))\n",
    "# #     print(np.var(trainer.testaccuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(range(1, trainer.epochs + 1), trainer.train_losses, label='Training losses')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(adj.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test Accuracies')\n",
    "print(trainer.testaccuracies)\n",
    "print('Train Losses')\n",
    "print(trainer.train_losses)\n",
    "print('Train Accuracies')\n",
    "print(trainer.trainaccuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
